{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%run 'commons.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing data to /home/ml/data\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting /home/ml/data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting /home/ml/data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /home/ml/data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /home/ml/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "path = \"/home/ml/data\"\n",
    "print (\"importing data to %s\" % path)\n",
    "mnist = input_data.read_data_sets(path, one_hot=True, reshape=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape [28,28]=784\n"
     ]
    }
   ],
   "source": [
    "image_width = 28\n",
    "image_height = 28\n",
    "image_size = np.multiply(image_width, image_height)\n",
    "print(\"image shape [%s,%s]=%s\" % (image_width, image_height, image_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe2e3092090>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# display image\n",
    "def display(imgs):\n",
    "    n=imgs.size / image_size\n",
    "    i=1\n",
    "    for img in imgs:\n",
    "        plt.subplot(1,n,i)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(img.reshape(28,28), cmap=cm.binary)\n",
    "        i=i+1\n",
    "\n",
    "# output images\n",
    "display(mnist.train.images[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "epochs = 1\n",
    "batch_size = 100\n",
    "display_epoch = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convolution\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first layer is a convolution, followed by max pooling. The convolution computes 32 features for each 5x5 patch. Its weight tensor has a shape of [5, 5, 1, 32]. The first two dimensions are the patch size, the next is the number of input channels (1 means that images are grayscale), and the last is the number of output channels. There is also a bias vector with a component for each output channel.\n",
    "\n",
    "\n",
    "To apply the layer, we reshape the input data to a 4d tensor, with the first dimension corresponding to the number of images, second and third - to image width and height, and the final dimension - to the number of colour channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf Graph Input\n",
    "# mnist data image of shape 28*28=784\n",
    "x = tf.placeholder(tf.float32, [None, 28, 28, 1], name='x')\n",
    "# 0-9 digits recognition => 10 classes\n",
    "labels = tf.placeholder(tf.float32, [None, 10], name='labels')\n",
    "\n",
    "# first convolutional layer, with max pool\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d(x, W_conv1) + b_conv1)\n",
    "#print (h_conv1.get_shape()) # => (40000, 28, 28, 32)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "#print (h_pool1.get_shape()) # => (40000, 14, 14, 32)\n",
    "\n",
    "# second convolutional layer, with max pool\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "# densely connected layer\n",
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "# drop out\n",
    "keep_prob = tf.placeholder_with_default(1.0, shape=(), name=\"keep_prob\")\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "# readout layer\n",
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "\n",
    "\n",
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=y_conv))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "y_softmax = tf.nn.softmax(y_conv, name='y_softmax')\n",
    "values, indices = tf.nn.top_k(y_softmax, 10)\n",
    "\n",
    "table = tf.contrib.lookup.index_to_string_table_from_tensor(tf.constant([str(i) for i in range(10)]))\n",
    "prediction_classes = table.lookup(tf.to_int64(indices))\n",
    "\n",
    "tf.summary.scalar(\"loss\", cross_entropy)\n",
    "tf.summary.scalar(\"accuracy\", accuracy)\n",
    "summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "cxt = Context(\"mnist-cnn\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total batches = 550\n",
      "step 0, training accuracy 0.09\n",
      "step 10, training accuracy 0.14\n",
      "step 20, training accuracy 0.34\n",
      "step 30, training accuracy 0.62\n",
      "step 40, training accuracy 0.68\n",
      "step 50, training accuracy 0.76\n",
      "step 60, training accuracy 0.77\n",
      "step 70, training accuracy 0.86\n",
      "step 80, training accuracy 0.76\n",
      "step 90, training accuracy 0.84\n",
      "step 100, training accuracy 0.84\n",
      "step 110, training accuracy 0.81\n",
      "step 120, training accuracy 0.87\n",
      "step 130, training accuracy 0.87\n",
      "step 140, training accuracy 0.87\n",
      "step 150, training accuracy 0.9\n",
      "step 160, training accuracy 0.94\n",
      "step 170, training accuracy 0.89\n",
      "step 180, training accuracy 0.96\n",
      "step 190, training accuracy 0.84\n",
      "step 200, training accuracy 0.87\n",
      "step 210, training accuracy 0.95\n",
      "step 220, training accuracy 0.94\n",
      "step 230, training accuracy 0.96\n",
      "step 240, training accuracy 0.89\n",
      "step 250, training accuracy 0.97\n",
      "step 260, training accuracy 0.95\n",
      "step 270, training accuracy 0.91\n",
      "step 280, training accuracy 0.93\n",
      "step 290, training accuracy 0.92\n",
      "step 300, training accuracy 0.93\n",
      "step 310, training accuracy 0.97\n",
      "step 320, training accuracy 0.96\n",
      "step 330, training accuracy 0.94\n",
      "step 340, training accuracy 0.94\n",
      "step 350, training accuracy 0.93\n",
      "step 360, training accuracy 0.93\n",
      "step 370, training accuracy 0.9\n",
      "step 380, training accuracy 0.94\n",
      "step 390, training accuracy 0.89\n",
      "step 400, training accuracy 0.97\n",
      "step 410, training accuracy 0.94\n",
      "step 420, training accuracy 0.91\n",
      "step 430, training accuracy 0.94\n",
      "step 440, training accuracy 0.9\n",
      "step 450, training accuracy 0.92\n",
      "step 460, training accuracy 0.95\n",
      "step 470, training accuracy 0.94\n",
      "step 480, training accuracy 0.96\n",
      "step 490, training accuracy 0.92\n",
      "step 500, training accuracy 0.93\n",
      "step 510, training accuracy 0.92\n",
      "step 520, training accuracy 0.92\n",
      "step 530, training accuracy 0.91\n",
      "step 540, training accuracy 0.93\n",
      "step 549, training accuracy 0.93\n",
      "Checkpoint saved to path: /home/ml/checkpoints/mnist-cnn/1/checkpoint.ckpt\n",
      "done training mnist-cnn\n",
      "INFO:tensorflow:No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: /home/ml/models/mnist-cnn/1/saved_model.pb\n",
      "Model exported to path: /home/ml/models/mnist-cnn/1\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # op to write logs to Tensorboard\n",
    "    summary_writer = tf.summary.FileWriter(cxt.log_path, graph=tf.get_default_graph())\n",
    "    \n",
    "    total_batches = int(mnist.train.num_examples/batch_size)\n",
    "    print(\"total batches = %s\" % (total_batches))\n",
    "    for e in range(epochs):\n",
    "        for i in range(total_batches):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size) \n",
    "            if i % 10 == 0:\n",
    "                # record summary data\n",
    "                train_accuracy, summary = sess.run([accuracy, summary_op], feed_dict={x: batch_xs, labels: batch_ys, keep_prob: 1.0})\n",
    "                summary_writer.add_summary(summary, e * total_batches + i)\n",
    "                print('step %d, training accuracy %g' % (i, train_accuracy))\n",
    "            else:\n",
    "                _ = sess.run([train_step], feed_dict={x: batch_xs, labels: batch_ys, keep_prob: 0.5})\n",
    "\n",
    "        print('step %d, training accuracy %g' % (i, train_accuracy))\n",
    "        p = saver.save(sess, \"%s/checkpoint.ckpt\" % (cxt.checkpoint_path))\n",
    "        print(\"Checkpoint saved to path: %s\" % p)\n",
    "\n",
    "    print('done training mnist-cnn')\n",
    "    \n",
    "    tensor_info_x = tf.saved_model.utils.build_tensor_info(x)\n",
    "    tensor_info_y = tf.saved_model.utils.build_tensor_info(y_softmax)\n",
    "\n",
    "    # build prediction signature\n",
    "    s = signature(x, y_softmax)\n",
    "\n",
    "    # saving model\n",
    "    legacy_init_op = tf.group(tf.tables_initializer(), name='legacy_init_op')\n",
    "        \n",
    "    builder = tf.saved_model.builder.SavedModelBuilder(cxt.model_path)\n",
    "    builder.add_meta_graph_and_variables(\n",
    "        sess,[tf.saved_model.tag_constants.SERVING],\n",
    "        signature_def_map = signature_def_map(s),\n",
    "        legacy_init_op=legacy_init_op)\n",
    "    \n",
    "    \n",
    "    builder.save()\n",
    "    print(\"Model exported to path: %s\" % cxt.model_path)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
